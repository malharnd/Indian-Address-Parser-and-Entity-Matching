{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#toeknized text\n","import transformers\n","from transformers import AutoTokenizer\n","from transformers import  DistilBertForTokenClassification\n","\n","import torch\n","import torch.nn as nn\n","\n","import torch\n","import re\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class DistilbertNER(nn.Module):\n","  \n","  def __init__(self, tokens_dim):\n","    super(DistilbertNER,self).__init__()\n","    \n","    if type(tokens_dim) != int:\n","            raise TypeError('Please tokens_dim should be an integer')\n","\n","    if tokens_dim <= 0:\n","          raise ValueError('Classification layer dimension should be at least 1')\n","\n","    self.pretrained = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = tokens_dim) #set the output of each token classifier = unique_lables\n","\n","\n","  def forward(self, input_ids, attention_mask, labels = None): #labels are needed in order to compute the loss\n","    #inference time no labels\n","    if labels == None:\n","      out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask )\n","      return out\n","\n","    out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask , labels = labels)\n","    return out"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Model and metadata loaded successfully.\n"]}],"source":["# Define the file path where the model is saved\n","model_save_path = \"distilbert_ner_model_meta.pth\"\n","\n","# Load the model data\n","model_data = torch.load(model_save_path, map_location=torch.device('cpu'))\n","\n","# Extract the model's state dictionary and metadata\n","model_state_dict = model_data[\"model_state_dict\"]\n","metadata = model_data[\"metadata\"]\n","idx2tag = metadata[\"idx2tag\"]\n","tag2idx = metadata[\"tag2idx\"]\n","\n","# Load the model class\n","model = DistilbertNER(len(metadata[\"unique_tags\"])) \n","\n","# Load the model's state dictionary\n","model.load_state_dict(model_state_dict)\n","\n","print(\"Model and metadata loaded successfully.\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["21 kartavya path near chandi chowkdelhi\n","['flat_apartment_number', 'street', 'street', 'landmark', 'landmark', 'landmark', 'pincode']\n"]}],"source":["import torch\n","import re\n","\n","def align_word_ids(texts):\n","    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n","    word_ids = tokenized_inputs.word_ids()\n","\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","        if word_idx is None:\n","            label_ids.append(-100)\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(1)\n","            except:\n","                label_ids.append(-100)\n","        else:\n","            try:\n","                label_ids.append(1 if label_all_tokens else -100)\n","            except:\n","                label_ids.append(-100)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","def evaluate_one_text(model, sentence):\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","    sentence_processed, pincode = preprocess_user_input(sentence)\n","    sentence =  sentence_processed\n","\n","    text = tokenizer(sentence, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n","    mask = text['attention_mask'].to(device)\n","    input_id = text['input_ids'].to(device)\n","    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n","\n","    logits = model(input_id, mask, None)\n","    logits_clean = logits[0][label_ids != -100]\n","\n","    predictions = logits_clean.argmax(dim=1).tolist()\n","    prediction_label = [idx2tag[i] for i in predictions]\n","\n","    if pincode:\n","        prediction_label.append('pincode')\n","        \n","\n","    print(sentence_processed)\n","    print(prediction_label)\n","\n","def preprocess_user_input(sentence):\n","    # Lowercase the input\n","    sentence = sentence.lower()\n","    # Remove commas\n","    sentence = sentence.replace(',', '')\n","    # Extract pincode using regex\n","    pincode = re.findall(r'\\b\\d{6}\\b', sentence)\n","    # Remove pincode from the sentence\n","    sentence = re.sub(r'\\b\\d{6}\\b', '', sentence)\n","    # Join the remaining text\n","    sentence = ' '.join(sentence.split())\n","    return sentence, pincode\n","\n","# Example usage:\n","sentence = \"21, Kartavya Path, near chandi chowk,Delhi 110001\"\n","evaluate_one_text(model, sentence)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4612925,"sourceId":7863230,"sourceType":"datasetVersion"},{"datasetId":4613051,"sourceId":7863446,"sourceType":"datasetVersion"},{"datasetId":4613539,"sourceId":7864212,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
